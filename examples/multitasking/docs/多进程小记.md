[TOC]

### 实验1-分析squid父进程和日志子进程之间的管道通信

#### 基本

- 使用`strace`、`ps`、`lsof`，观察启动时、启动后、通信时的管道系统调用
- **RTFM**。



#### 案例

```shell
# 1 查看进程基本关系
divsigma@tom:~/simple-server/examples/framework-intro$ ps -ef | grep squid
root       24982       1  0 00:39 ?        00:00:00 /usr/sbin/squid -sYC
proxy      24984   24982  0 00:39 ?        00:00:00 (squid-1) --kid squid-1 -sYC
proxy      24985   24984  0 00:39 ?        00:00:00 (logfile-daemon) /var/log/squid/access.log
divsigma   25708    5871  0 02:06 pts/2    00:00:00 grep --color=auto squid

# 2 启动telnet访问squid服务，并发送"hello"


# 3 telnet同时strace追踪日志守护进程
## 3.1 启动后，telnet squid服务，read(0,...的阻塞才会停止
divsigma@tom:~/simple-server/examples/framework-intro$ sudo strace -p 24985
strace: Process 24985 attached
read(0, "L1648951600.990      3 192.168.5"..., 4096) = 104
write(3, "1648951600.990      3 192.168.56"..., 101) = 101
## 3.2 用lsof进一步观察fd=0和fd=3是什么文件
### fd=0/1，被重定向到了同一个socket（怀疑是socketpair），而且0是读端
### fd=3，指向输出的日志文件，可以用vi打开，验证确实写入了上面的一行
divsigma@tom:~/simple-server/examples/framework-intro$ sudo lsof -p 24985
COMMAND     PID  USER   FD   TYPE             DEVICE SIZE/OFF   NODE NAME
log_file_ 24985 proxy    0u  unix 0xffff9e3aae889c00      0t0 178856 type=STREAM
log_file_ 24985 proxy    1u  unix 0xffff9e3aae889c00      0t0 178856 type=STREAM
log_file_ 24985 proxy    2u   CHR                1,3      0t0      6 /dev/null
log_file_ 24985 proxy    3w   REG              253,0      205 524295 /var/log/squid/access.log

# 4 telnet同时strace追踪主进程（日志进程的父进程）
divsigma@tom:~$ sudo strace -p 24984
strace: Process 24984 attached
epoll_wait(7, [], 65536, 790)       = 0
epoll_wait(7, [], 65536, 0)         = 0
epoll_wait(7, [{EPOLLIN, {u32=8, u64=8}}], 65536, 999) = 1
read(8, "hello\r\n", 4096)          = 7
getpid()                            = 24984
epoll_ctl(7, EPOLL_CTL_MOD, 8, {EPOLLIN|EPOLLOUT|EPOLLERR|EPOLLHUP, {u32=8, u64=8}}) = 0
epoll_wait(7, [{EPOLLOUT, {u32=8, u64=8}}], 65536, 273) = 1
write(8, "HTTP/1.1 400 Bad Request\r\nServer"..., 3673) = 3673
read(8, 0x7ffe9806b210, 65535)      = -1 EAGAIN (Resource temporarily unavailable)
epoll_ctl(7, EPOLL_CTL_ADD, 10, {EPOLLOUT|EPOLLERR|EPOLLHUP, {u32=10, u64=10}}) = 0
epoll_ctl(7, EPOLL_CTL_DEL, 8, 0x7ffe9807b484) = 0
close(8)                            = 0
epoll_wait(7, [{EPOLLOUT, {u32=10, u64=10}}], 65536, 269) = 1
write(10, "L1648951600.990      3 192.168.5"..., 104) = 104
epoll_wait(7, [{EPOLLOUT, {u32=10, u64=10}}], 65536, 266) = 1
epoll_ctl(7, EPOLL_CTL_DEL, 10, 0x7ffe9807b514) = 0
epoll_wait(7, [], 65536, 265)       = 0
epoll_wait(7, [], 65536, 0)         = 0
## 4.1 用lsof确认fd=7/8/10
### fd=7，是用于epoll_wait的fd；
### fd=10，根据DEVICE的地址和它在10上执行的write内容猜测
###        应该是用于跟子进程（日志进程）进行通信的socketpair写端（地址相差0x400）
### fd=9，不在lsof中，但观察strace输出，可见它是epoll_wait()返回的，而且从8中读取了“hello”，所以
###       8应该是进程用于和telnet通信的socket
divsigma@tom:~$ sudo lsof -p 24984
COMMAND   PID  USER   FD      TYPE             DEVICE SIZE/OFF   NODE NAME
squid   24984 proxy    0u      CHR                1,3      0t0      6 /dev/null
squid   24984 proxy    1u      CHR                1,3      0t0      6 /dev/null
squid   24984 proxy    2u      CHR                1,3      0t0      6 /dev/null
squid   24984 proxy    3u      REG              253,0     2603 524290 /var/log/squid/cache.log.1
squid   24984 proxy    4u      CHR                1,3      0t0      6 /dev/null
squid   24984 proxy    5u     IPv6             178853      0t0    UDP *:41812
squid   24984 proxy    6u     unix 0xffff9e3aae888000      0t0 178825 type=DGRAM
squid   24984 proxy    7u  a_inode               0,14        0  10376 [eventpoll]
squid   24984 proxy    9u     IPv4             178854      0t0    UDP *:59791
squid   24984 proxy   10u     unix 0xffff9e3aae889800      0t0 178855 type=STREAM
```





#### <span style="color:red">思考题</span>

##### Q1：直接`sudo strace service squid start`得到的输出为何找不到`fork`、`24984`和`24985`？如何跟踪squid启动过程中创建子进程和重定向时的系统调用？

描述：如题



#### Q2：`strace -p`实现的基本思路？

描述：系统调用会产生中断信号，strace捕获中断信号？







### 实验2-用POSIX接口共享内存demo

#### 基本

- 进程间通信（IPC）一般有四种方式：管道（常指双工管道）、信号量、消息队列、共享内存。
  - sockpair方式最原始简单，常用于父子进程通信（如实验1和之前的统一事件源）可以`man unix`查看更多（如实验3）
  - 后三种因为是由AT&T System V2版本的UNIX引入，所以统称为System V IPC。
  - <span style="color:red">各自优劣？</span>
- 共享内存针对**IPC**而言，有三种方式：mmap、SystemV的shm接口、POSIX的shm接口
  - <span style="color:red">三者使用方法？机制区别？优劣？</span>
- 可以尝试仿照实验1，用`lsof`和`ipcs`分析父子进程对mmap后文件的持有状态



#### 代码





#### <span style="color:red">思考题</span>

##### Q1：经典问题，线程vs进程？

描述：Linux2.6以前原生不支持线程，POSIX的线程库是怎么管理线程的（即NPTL）？相比进程模拟线程（如LinuxThread），有什么优缺点？

- https://labuladong.gitbook.io/algo-en/v.-common-knowledge/linuxprocess#:~:text=But%20both%20thread%20and%20process,shared%20with%20its%20parent%20process.
- https://www.informit.com/articles/article.aspx?p=370047&seqNum=3#:~:text=To%20the%20Linux%20kernel%2C%20there,certain%20resources%20with%20other%20processes.
- Linux2.6前不支持原生线程，LinuxThread中用`task_struct`管理线程，用clone系统调用并指定CLONE_THREAD创建线程，提供了管理线程，这相当于用进程模拟线程，有什么问题？
- 线程模型：完全用户调度（用户多线程对内核表现为单进程，内核用单线程调度该进程。早期Berkley UNIX采用该模式），完全内核调度（用户N线程映射为内核N线程。NPTL采用该模式），双层调度（N:M）。具体实现可用`getconf GNU_LIBPTHREAD_VERSION`查看。
- 









### 实验3-进程间传递文件描述符

#### 基本

- fd编号是局部的，为什么会有效？详见`man unix`的`SCM_RIGHTS`和`man open`的NOTES
  - 事实上，被传递的是文件描述（或者说文件对象）的引用，接收方会使用一个新的fd编号。文件描述（文件对象）记录了打开文件的各种信息，文件描述符是该对象的引用。
  - 还可以使用`kcmp()`的KCMP_FILE操作比较文件描述是否相等
    - 即可能有0==1！——`kcmp(pid_0, pid_1, KCMP_FILE, 0, 1)==0`
- 



#### 代码

- https://stackoverflow.com/questions/2358684/can-i-share-a-file-descriptor-to-another-process-on-linux-or-are-they-local-to-t



#### <span style="color:red">思考题</span>

##### Q1：关于文件描述符和设备的区别？为什么bash输入命令时看得见，输入password时看不见？如何实现？

描述：使用重定向似乎行不通

- 





### 实验4-子线程fork导致死锁

#### 基本

- 从`man fork`的NOTES中知道

  > Note the following further points:
  >
  > The  child **process is created with a single thread—the one that called fork()**.  The entire virtual address space of the parent is replicated in the child, including the **states of mutexes**, **condition variables, and other pthreads objects**; the use of pthread_atfork(3) may be  helpful  for  dealing with problems that this can cause.

  - 子进程只会从调用fork的线程上fork，所以**子进程是从父进程的main线程fork出来**的。因为**mutex是全局的**，子进程会继承mutex状态
  - 所以要让子进程的main线程进入死锁，就要在父进程的main线程fork之前，启动worker线程并获得锁（父进程在fork前先pthread_create并sleep一会儿），且未释放（worker获得锁后sleep，确保main线程fork时持有锁）。

- 最后状态

  - 子进程的main线程因为继承了父进程的worker线程中锁的状态，所以子进程的main线程死锁了。
  - 父进程的main线程因为wait(NULL)，也在等待
  - 父进程的worker线程不受影响。



#### 代码

- `multithread_fork.cc`

- 输出

  > divsigma@tom:~/simple-server/examples/multitasking$ ./multithread_fork
  > divsigma@tom:~/simple-server/examples/multitasking$ ./multithread_fork
  > [thread-140466592986880] trying to acquire lock
  > [thread-140466592986880] acquired lock
  > [main] waiting ...
  > [process-2780] trying to acquire lock
  > [thread-140466592986880] released lock
  >
  > NO RETURN

- 



#### <span style="color:red">思考题</span>

##### Q1：但`man fork`的DESCRIPTION又说子进程不会继承“process-associatd record locks”，mutex锁属于什么情况？

描述：如题。（为什么`man pthread_mutex_lock`也打不开？——POSIX手册在`manpages-posix-dev`包里）







### 实验5-用单一线程处理进程所有信号

#### 代码

- `signal_thread.cc`



#### <span style="color:red">思考题</span>

##### Q1：为什么`pthread_sigmask(SIG_BLOCK,...)`的信号能被`pthread_create`的线程`sigwait()`到？

描述：看不懂`man pthread_sigmask`的EXAMPLE。

- `man pthread_sigmask` --> `man sigprocmask` --> `man sigwait` --> `man sigwaitinfo`--> `man sigprocmask`--> `man sigwaitinfo`-->...

> **[man pthread_sigmask]**
>
> NOTES
>
> ​	A new thread inherits a copy of its creator's signal mask.
>
> 
>
> **[man sigprocmask]**
>
> SIG_BLOCK
>
> ​	The set of blocked signals is the union of the current set and the set argument.
>
> SIG_UNBLOCK
>               The signals in set are removed from the current set of blocked signals.  It is permissible to attempt to unblock a signal which is not blocked.
>
> ...
>
> NOTES
>
> ​	It is not possible to block SIGKILL or SIGSTOP.  Attempts to do so are silently ignored.
>
> ​	Each of the threads in a process has its own signal mask.
>
> ​	A child created via fork(2) inherits a copy of its parent's signal mask; the signal mask is preserved across execve(2).
>
> 
>
> **[man sigwaitinfo]**
>
> NOTES
>
> In normal usage, the calling program blocks the signals in set via a prior call to sigprocmask(2) (so that the default disposition for  these  signals does  not occur if they become pending between successive calls to sigwaitinfo() or sigtimedwait()) and does not establish handlers for these signals. 
>
> In a **multithreaded** program, the signal should be blocked in all threads, in order to prevent the signal being treated according to its default  disposition in a thread other than the one calling sigwaitinfo() or sigtimedwait()).
>
> The  set  of signals that is pending for a given thread is the union of the set of signals that is pending specifically for that thread and the set of signals that is pending for the process as a whole (see signal(7)).
>
> Attempts to wait for SIGKILL and SIGSTOP are silently ignored.
>
> If **multiple threads** of a process are blocked waiting for the same signal(s) in sigwaitinfo() or sigtimedwait(), then exactly one of the threads will actually receive the signal if it becomes pending for the process as a whole; which of the threads receives the signal is indeterminate.   

- 总之，细品`man`：
  - 每个线程都有自己的信号掩码，而且每个自己BLOCK的信号都有默认处理方式。`sigwait()`就是wait这些BLOCK的信号来**自定义处理**。所以多线程中为了统一BLOCK信号行为，应该先统一BLOCK（缺省各个线程的自定义处理），然后需要处理的线程调用`sigwait()`来处理。
  - 进程的每个线程都有信号掩码，并继承自创建它们的线程的信号掩码。通过`pthread_sigmask(SIG_BLOCK,...)`后的信号一般会调用`sigwait()`进行自定义处理，因为这些信号被掩盖后，默认没有处理函数（SIGKILL和SIGSTOP除外）。为了避免BLOCK的信号被`sigwait()`以外的线程默认处理了，一个进程的多个线程会同时BLOCK这些信号并默认不处理。最终最多只有一个调用了`sigwait()`的线程对BLOCK信号进行处理。



##### Q2：BLOCK==截获？如何查看信号默认处理的行为？





### 实验6-实现简单进程池

#### 基本

- 半同步/半异步反应堆模式：主进程监听m_listenfd，发现有连接请求后以Round-Robin方式找到一个子进程，用ipc_pipefd通知它，子进程`accept()`连接并进行非阻塞的读取和处理。
- 主进程关键文件描述符
  - IO复用文件描述符`m_epollfd`：其interest list包含`m_listenfd`、`ipc_pipefd[2]`和`sig_pipefd[2]`。
    - 需要处理信号到达时的EINTR（epoll_wait会阻塞，信号到达时先EINTR返回一次再返回`sig_pipefd[2]`）
  - 对外监听连接`m_listenfd`：
  - 进程间通信`ipc_pipefd[2]`：使用socketpair。仅用于通知子进程有新连接到达，但本实验代码中不传递`m_listenfd`。
  - 统一信号源`sig_pipefd[2]`：使用socketpair。将SIGTERM、SIGINT、SIGCHLD的处理统一到IO复用。
- 子进程关键文件描述符
  - IO复用文件描述符`m_epollfd`：其interest list包含`client[].m_sockfd`、`ipc_pipefd[2]`和`sig_pipefd[2]`。
    - `client[].m_sockfd`在加入`m_epollfd`前需要设置为非阻塞
  - 接受对外连接的`m_listenfd`：用于accept()得到`client_sockfd`，**从主进程dup/fork获取**。
  - 与连接后客户端通信的`client[].m_sockfd`：用于收发对客户端的数据
    - `client[].m_sockfd`在加入`m_epollfd`前需要设置为非阻塞
    - 需要处理非阻塞socket的EAGAIN（recv或write发现对非阻塞文件发生阻塞时，返回EAGAIN。详见`man recv`），用于跳出当前socket的处理循环。
    - **见`man epoll`的Example for suggested usage**
  - 进程间通信`ipc_pipefd[2]`：使用socketpair。仅用于接收主进程的通知。
  - 统一信号源`sig_pipefd[2]`：同主进程。



#### 代码

- `processpool.h`
- `test_processpool.cc`



#### <span style="color:red">思考题</span>

##### Q1：`listen()`、`accept()`、`recv()`、`write()`、`epoll_wait()`的阻塞行为是？

- 阻塞：accept、epoll_wait
- 不阻塞：listen
- 阻塞/不阻塞：recv、write（不阻塞时处理两个errno：EAGAIN或EWOULDBLOCK）



##### Q2：为什么socketpair没有O_NONBLOCK标志（用fcntl(fd, F_GETFL)获取），统一到IO后其send和recv看起来没有阻塞？



### 实验7-对比多进程和串行的http处理代码效率（注意workload和稳定复现）


#### 基本

- 多进程代码：
  - `processpool.h`
  - `nonblock_http_handler.h`
  - `test_multiproc_http_handler.cc`
- 串行代码
  - `test_block_http_handler.cc`



#### 问题1（20220718）

描述：为什么多进程代码（开5个处理进程）在VPS上`webbench -c 500`时会卡住（log不再更新）？串行代码则不会？

1、尝试在虚拟机Tom复现，不出现问题。



#### 问题2（20220718）：多进程代码无法连续工作

描述：为什么”多进程代码-1进程“在虚拟机Tom下连续多次`webbench -t 10 -c 2`时可能会卡住（无法继续处理），此时输出的log多表现为某处理进程在epoll_wait处阻塞。

- 20220718：最简稳定复现的情况

  ```shell
  # 情况1：第一次webbench后无法继续webbench
  ## 1子进程，backlog=5,
  (bash0)$ ./test_multiproc_http_handler 192.168.56.2 11111
  ## 第一次webbench
  (bash1)$ webbench -t 1 -c 2 http://192.168.56.2:11111/
  speed=1+pages/min
  ## 第一次webbench返回后，第二次webbench的QPS=0
  (bash1)$ webbench -t 1 -c 2 http://192.168.56.2:11111/
  speed=0pages/min
  
  # 情况2：第一次webbench后无法wget
  ## 1子进程，backlog=5,
  (bash0)$ ./test_multiproc_http_handler 192.168.56.2 11111
  ## 第一次webbench
  (bash1)$ webbench -t 1 -c 2 http://192.168.56.2:11111/
  speed=1+pages/min
  ## 第一次webbench后，wget一直在等待回复
  (bash1)$ wget http://192.168.56.2:11111/
  Request sent, waiting for Respond ...
  ## 观察1：对于情况2，wget之后bash0输出一个报文内容是“...Webbench 1.15...”的处理结果，此时运行下述命令，bash1输出一个报文内容是“...Wget...”的处理结果，而且bash1成功wget
  ## bash2承接bash1后的wget后再发起一次wget，此时bash1成功wget
  (bash2)$ wget http://192.168.56.2:11111/
  Request sent, waiting for Respond ...
  ```

- 分析工具

  - `tcpdump`抓包
  - `strace -f -e trace=network -tt`跟踪网络相关的系统调用（用于对应tcpdump包的结果）
  - `netstat -nat`查看第一次webbench后是否有异常链接（如CLOSE_WAIT）

- 抓包分析：`./log/202207201501-tcpdump`日志分析
  
  - 产生日志的命令
    
    ```shell
    $ sudo tcpdump -ntXi lo port 11111 > tcpdump_log # 输出到控制台的tcpdump好像不全？
    ```
    
  - 首先完成了一次试探性的报文（端口40336），端口40338先完成三次握手、传输请求报文、收到确认，接着端口40340完成三次握手、传输请求、收到确认，随后端口40338才收到回复、四次挥手。可见建立连接的端口n，在端口n+2建立连接并传输请求报文后，才收到回复并关闭链接。
  - 如果持续下去，在第二次webbench时，试探性报文用于触发处理上次webbench遗留的最后一个连接的响应，第二次webbench并发发起的两个连接则无法被触发处理。可能跟epoll_wait的触发机制有关。
  
- 系统调用分析：`./log/202207201501-strace`日志分析
  
  - 产生日志的命令
    
    ```shell
    $ sudo strace -f -e trace=network -s 1000 -tt \
    ./test_multiproc_http_handler 192.168.56.2 11111
    ```
    
  - 陆续的send(5)，recv(4)，accept，recv(8)，结合accept队列情况和`lsof`查看的文件描述符情况，可知道每次父进程跳出epoll_wait后调用send(5)通知子进程，子进程收到信号跳出epoll_wait执行recv(4)，然后accept一次，同时recv(8)读取TCP内核缓存中的数据，完成响应。
  
  - 将系统调用时序和抓包时序对应，发现
    - 子进程中的accept()依赖于父进程的send(5)，
    - webbench最开始的2个连接并发建立，请求报文传输完毕后，进入listen的backlog队列，仅触发1次父进程的sys_exit_epoll_wait（待straceepoll确定，但只触发了一次send(5)），从而只触发了子进程的1次accpet。
    - 所以`webbench -t 1 -c 2`的最后，会有一个建立连接并确认了请求报文的端口，但没有被accept。
  - 关于未accept能否建立TCP：https://cloud.tencent.com/developer/article/1891404
  - 关于TCP的send和recv机制（未调用recv时报文能否收到ACK）：https://www.ccppcoding.com/archives/173
  - 关于ET/LT、阻塞/非阻塞的accept
    - https://mp.weixin.qq.com/s/nacUx_qQr93_y-CsVxkejA
    - https://www.cnblogs.com/kex1n/p/7211175.html
  
- 解决方案1
  - 原代码中，listenfd非阻塞，父进程epoll边缘触发监听listenfd，触发后用socketpair通知子进程，子进程做1次accept()。这会导致并发进入backlog队列的多个短连接（三次握手后立刻传送了请求报文）只被触发1次accept()。
  - 改进：边缘触发的listenfd需要非阻塞式listenfd+循环accept



#### 问题3（20220718）：2个客户端并发时，多进程程序的QPS比串行程序的还小

描述：为什么“多进程代码-2进程”在虚拟机Tom下`webbench -t 10 -c 2`的QPS比串行的还小？
##### 20220718：测试

- 虚拟机Tom启动服务，虚拟机Tom作webbench
- 串行代码连续测试3次，平均为在61,000pages/min。
- 2进程代码连续测试3次，第一次51,000pages/min，第二第三次0pages/min。

##### 20220720：测试

- 虚拟机Tom启动服务，输出定向到/dev/null，虚拟机Tom作webbench
- 串行代码：连续测试3次，平均为770,000pages/min
- 多进程代码-1进程：连续测试3次，平均为533,000pages/min
- 多进程代码-2进程：连续测试3次，平均为510,000pages/min
- 多进程代码-4进程：连续测试3次，平均为447,000pages/min

##### 20220721：抓包和listen队列状态分析

- 分析工具

  - `strace -f -tt -e trace=network`
  - `netstat -s |grep -i listen`：采样监测listen队列溢出总数
  - `sudo ss -tlp`：采样监测listen队列实时容量
  - `sudo netstat -natp | grep <port>`：采样监测特定端口ESTABLISHED连接和进程绑定情况（只能看到自己创建的进程）

- `./log/202207210940-strace`日志分析

  - 配置：多进程代码-2进程、backlog为5、每个子进程最多循环100次accept、父进程监听`192.168.56.2:11111`

  - 产生日志的命令

    ```shell
    $ sudo strace -f -tt -s 1000 -e trace=network,epoll_wait,writev -o strace.out \
    ./test_multiproc_http_handler 192.168.56.2 11111
    ```

  - 多数时候，2个子进程交替串行地处理2个客户端的串行请求（相当于“逐进程串行”）。因当前场景下，webbench每个客户端都是串行地发送请求，所以一个子进程writev后，才触发CPU调度父进程跳出epoll_wait，然后父进程Round-Robin式通知下一个子进程接收处理。当前场景下，webbench各个客户端串行发送短连接，受限于程序处理请求的速度，backlog多数时候未被填满（并发量不足），此时引入多进程，相当于串行基础上引入了更多epoll_wait和IPC。

  - 可以用多个命令确定backlog状态：https://juejin.cn/post/6844903949221232647

  - 确认backlog状态-方法1：用`netstat -s`多次采样，观察backlog队列溢出情况

    ```shell
    ## 启动服务程序
    (bash0)$ ./test_block_http_handler 192.168.56.2 -p 11111
    ## 启动客户程序，2客户端并发
    (bash1)$ webbench -t 60 -c 2 http://192.168.56.2:11111/
    ## 采样监测，可见此时listen queue的溢出数量稳定
    (bash2)$ sudo netstat -s | grep -i listen
        113132 times the listen queue of a socket overflowed
        113132 SYNs to LISTEN sockets dropped
    (bash2)$ sudo netstat -s | grep -i listen
        113132 times the listen queue of a socket overflowed
        113132 SYNs to LISTEN sockets dropped
    (bash2)$ sudo netstat -s | grep -i listen
        113132 times the listen queue of a socket overflowed
        113132 SYNs to LISTEN sockets dropped
    ## 启动客户程序，100客户端并发
    (bash1)$ webbench -t 60 -c 100 http://192.168.56.2:11111/
    ## 采样监测，可见此时listen queue的溢出数量持续增加
    (bash2)$ sudo netstat -natps | grep -i listen
        115887 times the listen queue of a socket overflowed
        115887 SYNs to LISTEN sockets dropped
    (bash2)$ sudo netstat -natps | grep -i listen
        115926 times the listen queue of a socket overflowed
        115926 SYNs to LISTEN sockets dropped
    (bash2)$ sudo netstat -natps | grep -i listen
        115952 times the listen queue of a socket overflowed
        115952 SYNs to LISTEN sockets dropped
    ```

  - 确认backlog状态-方法2：用`sudo ss -tlp`多次采样，直接查看backlog状态（`netstat`似乎做不到？`-p`选项不会列出不属于用户的进程，详见`man netstat`和`man ss`），处于监听状态的socket的Send-Q为backlog参数，Recv-Q为backlog队列中ESTABLISH数量。内核实际的backlog队列大小为“backlog参数+1”

    ```shell
    ## 启动服务程序
    (bash0)$ ./test_block_http_handler 192.168.56.2 -p 11111
    ## 启动客户程序，2客户端并发
    (bash1)$ webbench -t 60 -c 2 http://192.168.56.2:11111/
    ## 采样监测，可见Send-Q稳定地大于Recv-Q
    (bash2)$ sudo ss -tlp
    LISTEN  1  5  192.168.56.2:11111  0.0.0.0:*  users:(("test_block_http",pid=103834,fd=3))
    (bash2)$ sudo ss -tlp
    LISTEN  1  5  192.168.56.2:11111  0.0.0.0:*  users:(("test_block_http",pid=103834,fd=3))
    (bash2)$ sudo ss -tlp
    LISTEN  1  5  192.168.56.2:11111  0.0.0.0:*  users:(("test_block_http",pid=103834,fd=3))
    ## 启动客户程序，100客户端并发
    (bash1)$ webbench -t 60 -c 100 http://192.168.56.2:11111/
    ## 采样监测，可见Send-Q ≤ Recv-Q，listen queue很有可能溢出了
    (bash2)$ sudo ss -tlp
    LISTEN  6  5  192.168.56.2:11111  0.0.0.0:*  users:(("test_block_http",pid=108200,fd=3))
    (bash2)$ sudo ss -tlp
    LISTEN  5  5  192.168.56.2:11111  0.0.0.0:*  users:(("test_block_http",pid=108200,fd=3))
    (bash2)$ sudo ss -tlp
    LISTEN  5  5  192.168.56.2:11111  0.0.0.0:*  users:(("test_block_http",pid=108200,fd=3))
    ```

  - 确认backlog状态-方法3：用`sudo netstat -natp | grep ESTABLISH`采样监测完成三次握手的连接和进程的绑定情况（有一些难以理解的情况。。。）

    ```shell
    ## 启动服务程序
    (bash0)$ ./test_block_http_handler 192.168.56.2 -p 11111
    ## 启动客户程序，2客户端并发
    (bash1)$ webbench -t 60 -c 2 http://192.168.56.2:11111/
    ## 第一次查看，存在客户端对服务端的单向ESTABLISHED
    (bash2)$ sudo netstat -natp | sudo grep ESTABLISH
    tcp    0      0 192.168.56.2:22         192.168.56.1:7942       ESTABLISHED 96894/sshd: divsigm 
    tcp    0      0 127.0.0.1:33021         127.0.0.1:51988         ESTABLISHED 97072/node
    ...
    tcp    0      0 192.168.56.2:57402      192.168.56.2:11111      ESTABLISHED -
    tcp    0      0 192.168.56.2:22         192.168.56.1:5565       ESTABLISHED 87225/sshd: divsigm
    tcp    0      0 192.168.56.2:57580      192.168.56.2:11111      ESTABLISHED -                   
    tcp    0      0 192.168.56.2:57608      192.168.56.2:11111      ESTABLISHED -
    ## 第二次查看，存在服务端对客户端的单向ESTABLISHED
    (bash2)$ sudo netstat -natp | grep ESTABLISH
    tcp    0      0 192.168.56.2:41600      192.168.56.2:11111      ESTABLISHED -                   
    tcp    0      0 192.168.56.2:41630      192.168.56.2:11111      ESTABLISHED -                   
    tcp    0      0 192.168.56.2:41734      192.168.56.2:11111      ESTABLISHED -                   
    tcp    0      0 192.168.56.2:11111      192.168.56.2:41810      ESTABLISHED -                   
    tcp    0      0 192.168.56.2:22         192.168.56.1:7942       ESTABLISHED 96894/sshd: divsigm 
    tcp    0      0 192.168.56.2:41828      192.168.56.2:11111      ESTABLISHED -                   
    tcp    0      0 192.168.56.2:11111      192.168.56.2:41826      ESTABLISHED -
    tcp    0      0 127.0.0.1:51988         127.0.0.1:33021         ESTABLISHED 97019/sshd: divsigm 
    tcp    0     45 127.0.0.1:33021         127.0.0.1:51986         ESTABLISHED 1139/node
    tcp    0      0 192.168.56.2:41904      192.168.56.2:11111      ESTABLISHED -
    ...
    tcp    0      0 192.168.56.2:11111      192.168.56.2:41902      ESTABLISHED -
    ## 第三次查看，存在双向ESTABLISHED，但Send-Q有的是64（抓包可知，当前webbench请求报文长度为64），有的是0
    (bash2)$ sudo netstat -natp | grep ESTABLISH
    tcp    0      0 192.168.56.2:49734      192.168.56.2:11111      ESTABLISHED -                   
    ...
    tcp   64      0 192.168.56.2:11111      192.168.56.2:49734      ESTABLISHED - 
    tcp    0      0 192.168.56.2:53450      192.168.56.2:11111      ESTABLISHED -
    ...
    tcp    0      0 192.168.56.2:11111      192.168.56.2:53450      ESTABLISHED -
    ```

- 根据`./log/202207210940-strace`日志分析，觉得可能是请求数量不够多，未能体现多进程的并发处理能力。于是尝试提高并发量，可以看到约200并发时，backlog被填满。但此时“多进程代码-2进程”的QPS依旧不如串行程序，**并且出现问题4**

##### 20220723：CPU负载和上下文切换分析

- 分析工具

  - `uptime`：查看CPU平均负载（过去一段时间内runnable或uninterruptable进程的平均数）。
  - `sudo vmstat [interval]`：看CPU和IO基本情况。看`r`列是否超过CPU核数、`cs`数量、CPU用于`us`/`sy`/`wa`的占比情况。
  - `sudo pidstat -w -u [interval]`/`top -u <username>`：`-w`参数看进程上下文切换情况，`-u`看进程CPU使用的情况（即进程的`vmstat`）
  - `sudo perf top -p <pid> --no-children`：实时显示`sudo perf record -p <pid> --no-children`结果，找出特定进程在各函数上的CPU耗时占比
  - 

- “串行程序”的上下文切换分析

  - 配置：backlog为5

  - 测试命令

    ```shell
    (bash0)$ ./test_block_http_handler 1921.168.56.2 -p 11111 > /dev/null
    (bash1)$ webbench -t 30 -c 2 http://192.168.56.2:11111/
    ```

  - 思路：`uptime`和`vmstat`查看大致的CPU和IO情况，`pidstat`查看进程的CPU和IO情况，`perf top`/`perf record`确认瓶颈

  - 多次`uptime`和`vmstat`查看负载（此时系统为单核CPU）

    ```shell
    (bash2)$ uptime
     04:30:51 up 2 days, 54 min,  1 user,  load average: 0.03, 0.17, 0.24
     04:31:04 up 2 days, 54 min,  1 user,  load average: 0.33, 0.23, 0.26
     04:31:18 up 2 days, 55 min,  1 user,  load average: 0.43, 0.26, 0.27
     04:31:21 up 2 days, 55 min,  1 user,  load average: 0.48, 0.27, 0.27
     04:31:25 up 2 days, 55 min,  1 user,  load average: 0.52, 0.28, 0.27
     
    (bash2)$ sudo vmstat 2
    procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
     r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
     0  0  38400 802992  77772 608400    0    2    13    13   98  234  1  1 97  0  0
     0  0  38400 803492  77772 608400    0    0     0     0  158  376  3  3 95  0  0
     0  0  38400 802484  77776 608400    0    0     0    14  210  559  2  4 94  1  0
     2  0  38400 800408  77776 608576    0    0     0     0  268 19806  8 41 51  0  0
     2  0  38400 800196  77784 608568    0    0     0     8  307 47921 19 81  0  0  0
     2  0  38400 799188  77784 608544    0    0     0     0  295 46673 15 85  0  0  0
     1  0  38400 799944  77784 608576    0    0     0     0  282 45540 15 85  0  0  0
     2  0  38400 799244  77792 608480    0    0     0     8  319 46159 20 80  0  0  0
     1  0  38400 799772  77792 608496    0    0     0     0  306 46902 20 81  0  0  0
     2  0  38400 799676  77796 608588    0    0     0     8  297 48619 15 85  0  0  0
    ```

    - 可见启动测试后，最近1分钟的平均负载一直在上升，最后稳定在0.52，似乎可以接受。转而用更细致的`vmstat`监测系统CPU和IO情况。可见启动测试后，`cs`大量增加，`sy`的占比高达80%，判断出此时系统的进程上下文切换增多，CPU主要耗时在系统调用，而且`r`比核数多1，判断出此时有进程在争用CPU，与`cs`增加的现象相符

  - 用`pidstat`观察各进程的上下文切换和CPU在`us`和`sy`的耗时占比（下面的结果只列出了webbench和串行服务程序的进程）

    ```shell
    # 查看“进程的vmstat”
    (bash2)$ sudo pidstat -w -u 4
    Linux 5.4.0-122-generic (tom)   07/23/2022      _x86_64_        (1 CPU)
    
    04:09:44 AM   UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
    04:09:48 AM  1000    240010    8.00   22.75    0.00   62.25   30.75     0  test_block_http
    04:09:48 AM  1000    240329    2.00   27.75    0.00    4.00   29.75     0  webbench
    04:09:48 AM  1000    240330    3.00   27.00    0.00    4.75   30.00     0  webbench
    
    04:09:44 AM   UID       PID   cswch/s nvcswch/s  Command
    04:09:48 AM  1000    240010    379.25  20989.25  test_block_http
    04:09:48 AM  1000    240329  10729.50    150.00  webbench
    04:09:48 AM  1000    240330  10569.25    197.00  webbench
    
    ...
    
    04:09:52 AM   UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
    04:09:56 AM  1000    240010    9.75   23.00    0.00   63.00   32.75     0  test_block_http
    04:09:56 AM  1000    240329    3.00   28.00    0.00    6.75   31.00     0  webbench
    04:09:56 AM  1000    240330    2.75   28.25    0.00    5.75   31.00     0  webbench
    
    04:09:52 AM   UID       PID   cswch/s nvcswch/s  Command
    04:09:56 AM  1000    240010    676.00  21399.00  test_block_http
    04:09:56 AM  1000    240329  10811.25    137.75  webbench
    04:09:56 AM  1000    240330  10973.75    362.00  webbench
    
    # 对串行服务程序做perf top
    (bash2)$ sudo perf top -p 240010 --no-children
    Samples: 9K of event 'cpu-clock:pppH', 4000 Hz, Event count (approx.): 745281544 lost: 0/0 drop: 0/0
    Overhead  Shared Object            Symbol
      32.71%  [kernel]                 [k] finish_task_switch
       6.86%  [kernel]                 [k] __lock_text_start
       4.56%  libc-2.31.so             [.] __nss_database_lookup
       3.35%  libc-2.31.so             [.] psiginfo
       1.68%  libc-2.31.so             [.] _IO_file_xsputn
       1.37%  [kernel]                 [k] do_syscall_64
       1.18%  libc-2.31.so             [.] puts
       1.11%  libc-2.31.so             [.] __close
       0.99%  [kernel]                 [k] kmem_cache_free
       0.79%  [kernel]                 [k] __tcp_transmit_skb
       0.76%  [kernel]                 [k] ip_finish_output2
       0.75%  libc-2.31.so             [.] _IO_default_xsputn
       0.73%  test_block_http_handler  [.] RespondHelloHTTP
       0.70%  libc-2.31.so             [.] writev
       0.66%  test_block_http_handler  [.] main
       0.66%  [kernel]                 [k] __softirqentry_text_start
       0.66%  [kernel]                 [k] __kmalloc_node_track_caller
       0.62%  libc-2.31.so             [.] accept
       0.62%  test_block_http_handler  [.] CheckOneLine
       0.61%  [kernel]                 [k] copy_user_generic_string
       0.59%  libc-2.31.so             [.] recv
    
    # 对webbench的进程做perf top
    (bash2)$ sudo perf top -p 240329 --no-children
    Samples: 2K of event 'cpu-clock:pppH', 4000 Hz, Event count (approx.): 123120958 lost: 0/0 drop: 0/0
    Overhead  Shared Object     Symbol
      27.18%  [kernel]          [k] finish_task_switch
       2.35%  libc-2.31.so      [.] read
       2.19%  [kernel]          [k] kmem_cache_free
       1.65%  [kernel]          [k] do_syscall_64
       1.26%  [kernel]          [k] __lock_text_start
       1.22%  [kernel]          [k] fib_table_lookup
       1.16%  [kernel]          [k] memset
       1.11%  [kernel]          [k] kfree
       1.06%  [kernel]          [k] kmem_cache_alloc
       1.02%  [kernel]          [k] __tcp_transmit_skb
       0.95%  [kernel]          [k] __kmalloc_node_track_caller
       0.86%  [kernel]          [k] tcp_ack
       0.77%  [kernel]          [k] loopback_xmit
       0.75%  [kernel]          [k] kmem_cache_alloc_node
       0.74%  [kernel]          [k] read_tsc
       0.70%  [kernel]          [k] __call_rcu
       0.70%  [kernel]          [k] __inet_lookup_established
       0.65%  [kernel]          [k] ip_finish_output2
       0.64%  libc-2.31.so      [.] __close
       0.60%  [kernel]          [k] memcg_kmem_get_cache
       0.60%  libc-2.31.so      [.] __connect
    ```

    - 串行服务程序比较稳定地表现为`%wait`占比高，`%system`约为`%usr`占比的2倍，发生多次`nvcswch`（时间片到期导致的非自愿上下文切换）。发生多次`nvcswch`与`%wait`占比高的结果相符（通过手册发现，`pidstat`的`%wait`指进程用于等待CPU的时间，区别于`vmstat`/`iowait`中用来等待IO的`wa`），说明串行程序的多数时间用来等待CPU资源和用来做系统调用了。通过`perf top -p <pid>`也可以看到，串行服务程序的多数CPU时间花在了`[k] finish_task_switch`，也就是上下文切换上。
    - webbench程序比较稳定地表现为`%system`约为`%usr`占比的10倍，发生多次`cswch`（因等待资源而产生的资源上下文切换）。通过`perf top -p <pid>`也可以看到，webbench的2个并发客户端的多数CPU时间花在了`[k] finish_task_switch`和`[.] read`上，也就是在做上下文切换或等待服务程序响应。
    - 可见，串行服务程序在等CPU资源、webbench在等串行程序回复，CPU多数时间应该都用到进程上下文切换上了。

- “多进程代码-2进程”的上下文切换分析

  - 配置：backlog为5、每个子进程最多循环100次accept

  - 测试命令

    ```shell
    (bash0)$ ./test_multiproc_http_handler 192.168.56.2 11111 > /dev/null
    (bash1)$ webbench -t 30 -c 2 http://192.168.56.2:11111/
    ```

  - 思路：同“串行代码”上下文切换分析

  - 总体结果

    ```shell
    (bash2)$ uptime
     06:52:01 up 2 days,  3:15,  1 user,  load average: 0.09, 0.06, 0.08
     06:52:13 up 2 days,  3:16,  1 user,  load average: 0.24, 0.09, 0.09
     06:52:18 up 2 days,  3:16,  1 user,  load average: 0.38, 0.12, 0.10
     06:52:30 up 2 days,  3:16,  1 user,  load average: 0.63, 0.18, 0.12
     06:52:38 up 2 days,  3:16,  1 user,  load average: 0.92, 0.26, 0.15
     06:52:42 up 2 days,  3:16,  1 user,  load average: 0.85, 0.25, 0.15
    
    (bash2)$ sudo vmstat 4
    procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
     r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
     0  0  38400 508172  88980 616588    0    0     0     0  195  436  3  3 94  0  0
     3  0  38400 506736  88984 616644    0    0     0     8  260 2739  7  9 84  0  0
     2  0  38400 506516  88984 616648    0    0     0     0  330 35585 25 75  0  0  0
     3  0  38400 507508  88992 616664    0    0     0     0  310 37196 25 76  0  0  0
     3  0  38400 508084  89000 616616    0    0     0   100  294 38195 24 76  0  0  0
     0  0  38400 507968  89032 616568    0    0     0     8  307 32178 26 65  9  0  0
    
    (bash2)$ sudo pidstat -w -u 4
    Linux 5.4.0-122-generic (tom)   07/23/2022      _x86_64_        (1 CPU)
    
    06:57:26 AM   UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
    06:57:30 AM  1000    245745    3.00    7.00    0.00    1.25   10.00     0  test_multiproc_
    06:57:30 AM  1000    245746    6.75   14.25    0.00   17.00   21.00     0  test_multiproc_
    06:57:30 AM  1000    245747    8.25   12.75    0.00   17.50   21.00     0  test_multiproc_
    06:57:30 AM  1000    246535    1.50   19.75    0.00   64.25   21.25     0  webbench
    06:57:30 AM  1000    246536    1.50   19.50    0.00   65.25   21.00     0  webbench
    
    06:57:26 AM   UID       PID   cswch/s nvcswch/s  Command
    06:57:30 AM  1000    245745   7086.00     56.25  test_multiproc_
    06:57:30 AM  1000    245746   6054.00    153.00  test_multiproc_
    06:57:30 AM  1000    245747   6049.75    166.50  test_multiproc_
    06:57:30 AM  1000    246535   3450.25   3788.75  webbench
    06:57:30 AM  1000    246536   3372.25   3750.25  webbench
    
    ...
    
    06:57:46 AM   UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
    06:57:50 AM  1000    245745    3.00    7.00    0.00    1.25   10.00     0  test_multiproc_
    06:57:50 AM  1000    245746    7.50   13.75    0.00   16.50   21.25     0  test_multiproc_
    06:57:50 AM  1000    245747    7.75   13.25    0.00   16.50   21.00     0  test_multiproc_
    06:57:50 AM  1000    246535    1.75   19.50    0.00   65.25   21.25     0  webbench
    06:57:50 AM  1000    246536    2.00   19.50    0.00   64.75   21.50     0  webbench
    
    06:57:46 AM   UID       PID   cswch/s nvcswch/s  Command
    06:57:50 AM  1000    245745   7312.25     39.00  test_multiproc_
    06:57:50 AM  1000    245746   6392.75    103.50  test_multiproc_
    06:57:50 AM  1000    245747   6383.25    100.50  test_multiproc_
    06:57:50 AM  1000    246535   3441.00   3941.00  webbench
    06:57:50 AM  1000    246536   3460.00   3902.25  webbench
    
    # perf top的结果类似“串行代码”上下文切换分析，CPU时间主要花在[k] finish_task_switch上
    ```

    - `uptime`的负载相比串行时有增加，这可能是因为单核机器进程数增多了
    - `vmstat`中`r`增加到了3，而此时只有单核CPU，可知CPU争用情况更严重了，`cs`/`us`/`sy`的结果也能印证这一点
    - `pidstat`可以看到，webbench等待CPU资源的时间也增多了，这可能也会限制“多进程代码-2进程”的QPS（发包速率没有“串行代码”的快）。通过`perf top`可以看到，CPU时间还是主要花在`[k] finish_task_switch`上。

- 根据分析，尝试增加虚拟机Tom的CPU核数为2再做测试，或双机测试

  - 通过`/proc/cpuinfo`查看CPU信息，注意区分物理个数和核数：https://blog.51cto.com/u_12796481/2791138

  - 配置：backlog为5、“多进程代码”中每个子进程最多循环100次accept

  - 测试命令

    ```shell
    # 服务端命令
    $ ./test_multiproc_http_handler 192.168.56.2 11111 > /dev/null
    $ ./test_block_http_handler 192.168.56.2 11111 > /dev/null
    # 客户端命令
    $ webbench -t 30 -c <并发量> http://192.168.56.2:11111/
    ```

  - 双核单机的测试结果（单位为pages/min）

    | 测试程序\\并发量 | 1       | 2       | 5         | 10      | 20      |
    | ---------------- | ------- | ------- | --------- | ------- | ------- |
    | 串行代码         | 645,984 | 869,694 | 1,035,516 | 981,000 | 967,980 |
    | 多进程代码-1进程 | 421,050 | 553,470 | 757,842   | 794,970 | 700,296 |
    | 多进程代码-2进程 | 466,644 | 704,388 | 737,628   | 776,700 | 713,346 |

    - 单机测试可能不太公平，因为随着并发量上升，CPU核数测试程序和webbench同时使用，测出来的QPS可能还是受到CPU瓶颈限制
    - 可以看到`vmstat`中`r`列会达到4+，而此时只有双核
    - 所以尝试双机的测试（单机测试流量走lo网卡，似乎没有`[k] e1000_xmit_frame`和`[k] e1000_clean`的开销，所以双机的`perf`结果会与单机`perf`结果不同）

  - 双核Tom运行服务代码、单核Jerry运行webbench的测试结果（单位为pages/min）

    | 测试程序\\并发量 | 1      | 2       | 5       | 10      | 20      | 50      |
    | ---------------- | ------ | ------- | ------- | ------- | ------- | ------- |
    | 串行代码         | 82,764 | 131,004 | 134,136 | 60,024  | 43,680  | 36,798  |
    | 多进程代码-1进程 | 80,580 | 115,722 | 127,548 | 123,006 | 124,758 | 122,874 |
    | 多进程代码-2进程 | 81,510 | 110,778 | 131,910 | 123,036 | 124,806 | 126,846 |

    | 测试程序\\并发量 | 100     | 200     | 500     | 1000    | 2000    | 2500    | 3000   |
    | ---------------- | ------- | ------- | ------- | ------- | ------- | ------- | ------ |
    | 串行代码         | 56,466  | 37,854  | 43,422  | 38,772  | 27,738  | 26,370  | 16,602 |
    | 多进程代码-1进程 | 123,414 | 126,642 | 132,084 | 131,706 | 7266    | 6948    | 6948   |
    | 多进程代码-2进程 | 121,674 | 128,481 | 134,316 | 155,646 | 172,080 | 172,680 | 14,448 |

    - 单核Tom运行服务代码、单核Jerry运行webbench的测试结果，类似上述两表：可能之前产生的进程切换开销主要是webbench带来的（通过在Jerry上`vmstat`，可见并发量为10时，`r`列就飙升到6+）

    - 现象1：“串行代码”在并发量为10时，QPS有骤减

      - 猜想1-1：writev和close开销增大。

        对比并发量为5和并发量为10时，`strace -c`耗时统计，可见accept的平均耗时相差不大（3us），writev和close的平均耗时则增加了20us。但为什么并发量上来了，writev和close花费的时间就多了？。

        ```shell
        # “串行代码”并发量为10时的耗时统计
        (tom)$ sudo strace -p 3738 -c
        strace: Process 3738 attached
        ^Cstrace: Process 3738 detached
        % time     seconds  usecs/call     calls    errors syscall
        ------ ----------- ----------- --------- --------- ----------------
         40.45    0.928260         106      8686           writev
         35.13    0.806073          92      8687           close
         14.46    0.331835          38      8687           accept
          8.75    0.200779          23      8687           recvfrom
          1.21    0.027708          15      1804           write
        ------ ----------- ----------- --------- --------- ----------------
        100.00    2.294655                 36551           total
        
        # “串行代码”并发量为5时的耗时统计
        (tom)$ sudo strace -p 3738 -c
        strace: Process 3738 attached
        ^Cstrace: Process 3738 detached
        % time     seconds  usecs/call     calls    errors syscall
        ------ ----------- ----------- --------- --------- ----------------
         41.19    1.883946          86     21897           writev
         34.46    1.576361          71     21897           close
         15.84    0.724655          33     21897           accept
          7.16    0.327694          14     21897           recvfrom
          1.34    0.061470          13      4550           write
        ------ ----------- ----------- --------- --------- ----------------
        100.00    4.574126                 92138           total
        ```

    - 现象2：“串行代码”在QPS骤减前的并发量场景下，QPS与“多进程代码”接近

      - 猜想2-1：当前场景下，程序瓶颈主要在writev和close这两个系统调用，可见两份代码在并发量为5和10时，writev和close的耗时差别不大，而且都是最高的。即瓶颈在写IO上。

        

        “串行代码”中是阻塞写，“多进程代码”中是非阻塞写（通过fcntl()确认），但二者居然差异不大：

        （a）看`man 2 send`、`man 2 write`、`man 7 pipe`和`man 2 writev`，猜想因为**IO都是有缓冲区**的，write和writev的阻塞模式都类似管道的阻塞模式——触发阻塞条件是“是否向满载的写缓冲写数据”。本场景中程序响应只有66B，系统1个socket的tcp缓冲区最小是4096B（`/proc/sys/net/ipv4/tcp_wmen`）。正是因为IO数据量太小，所以不论有无设置阻塞socket，writrev都不会阻塞。

        

        影响writev系统调用时间的因素可能较多，还有很多疑惑：

        （a）网络带宽跑满了？通过`iperf`确认过，局域网网卡上下行带宽都能有2000Mb/s，而通过`iftop`监测发webbench测试时上下行的传输速度最高都只有5Mb/s左右。

        （b）全连接队列溢出有影响？通过`netstat -s | grep -i listen`可以知道，两份代码此时的连接队列是有溢出的，尝试增大backlog，“串行代码”（backlog为20）在并发量为10和20时的QPS接近“多进程代码”（backlog为5）在此场景下的表现，listen队列也没有溢出了。但从并发量为50开始，“串行代码”的QPS再次下滑。
        
        （c）单核webbench无法模拟大量的并发请求？
        
        但可以看到，相比“串行代码”，“多进程代码”通过**非阻塞accept+epoll_wait事件触发式并发处理**，减少了accept的耗时（大概少了10us），所以在“串行代码”QPS骤降的并发场景下，这种方式在相同时间内能accept并处理更多的请求，从而提升了QPS。
        
        ```shell
        # “多进程代码-1进程”并发量为5时的耗时统计
        (tom)$ sudo strace -p 3881 -c
        strace: Process 3881 attached
        ^Cstrace: Process 3881 detached
        % time     seconds  usecs/call     calls    errors syscall
        ------ ----------- ----------- --------- --------- ----------------
         41.46    1.716159          98     17481           writev
         33.62    1.391458          79     17481           close
          7.04    0.291250          12     23504      6021 accept
          6.47    0.267610          11     23503           recvfrom
          5.51    0.228237           6     34964           epoll_ctl
          3.77    0.156242           2     69931           fcntl
          1.16    0.048041           6      7152           epoll_wait
          0.97    0.040333           7      5221           write
        ------ ----------- ----------- --------- --------- ----------------
        100.00    4.139330                199237      6021 total
        
        # “多进程代码-1进程”并发量为10时的耗时统计
        (tom)$ sudo strace -p 3881 -c
        strace: Process 3881 attached
        ^Cstrace: Process 3881 detached
        % time     seconds  usecs/call     calls    errors syscall
        ------ ----------- ----------- --------- --------- ----------------
         42.77    2.352689         103     22659           writev
         33.63    1.849871          81     22659           close
          6.25    0.344037          11     29753           recvfrom
          6.25    0.344024          11     29757      7095 accept
          5.42    0.298223           6     45321           epoll_ctl
      3.57    0.196470           2     90651           fcntl
          1.09    0.059706           7      8209           epoll_wait
      1.02    0.056116           8      6737           write
        ------ ----------- ----------- --------- --------- ----------------
        100.00    5.501136                255746      7095 total
        ```
    
    - “多进程代码-1进程”在并发量为2000时，QPS有骤减，骤减后QPS水平远低于“串行代码”：
    
      - 现象：“多进程代码-1进程”在并发量为2000时，运行一小段时间后，程序在`top`中没有CPU占比了。此时用`strace -p <pid>`分析若干个webbench进程和“多进程代码-1进程”，发现进程在互相等待（epoll_wait或connect），但此时通过`ss -lt`看到，全连接队列是满的，为什么此时epoll_wait会阻塞？而此时“多进程代码-1进程”多数时候不阻塞？
      - 分析：“多进程代码-1进程”中，每个进程最多能accept并同时处理100个连接，accept是通过父进程经由管道发字符提醒子进程做的，listenfd是**边缘触发**。一旦请求的处理速度跟不上，子进程完成100个accept的同时，全连接队列已被填满了，此时父进程不会通知子进程继续accept，导致阻塞。可以看到，此时如果中断webbench，`netstat -nat`中会有backlog+1个CLOSE_WAIT的连接，可能是队列中没有被accept的。此时用`wget`尝试请求，会发现卡在了连接阶段（而非等待响应）。
      - 解决：父进程**水平触发**listenfd，将父子进程**IPC设为非阻塞**。若不设置成阻塞，可以从`strace`中看到父进程经过若干次IPC后卡在IPC的sendto无法返回。这时看到有backlog+1个连接已经ESTABLISHED，webbench退出后，这几个连接变为CLOSE_WAIT（即发送了`[F]`在等待响应），随后给父进程发送一个信号或中断`strace`，（**中断`strace`时会产生中断信号？中断信号都会触发对应进程的epoll_wait或阻塞API的一次返回（如`write`）？**），利用`tcpdump`抓包并`strace`子进程，可以看到子进程连续accept了6次，代码会响应这些CLOSE_WAIT的连接，然后这些连接资源能正常释放。这说明此时socketpair管道的内核缓存已经填满，导致父进程阻塞在管道写端；子进程对IPC是边缘触发，下一次进入epoll_wait前父进程已经阻塞，所以子进程阻塞在epoll_wait；父进程阻塞在写端，自然无法响应webbench的阻塞式connect。

##### 20220724：水平触发listenfd+非阻塞IPC的多进程代码测试

- 测试1：单核Tom运行服务、单核Jerry运行webbench。服务程序的backlog为5，子进程每次收到IPC最多循环accept100次。

  | 测试程序\\并发量 | 1      | 2       | 5       | 10     | 20      | 50      |
  | ---------------- | ------ | ------- | ------- | ------ | ------- | ------- |
  | 串行代码         | 69,324 | 115,092 | 123,714 | 30,444 | 36,618  | 25,284  |
  | 多进程代码-1进程 | 57,846 | 78,360  | 98,400  | 97,386 | 84,024  | 85,140  |
  | 多进程代码-2进程 | 68,556 | 94,242  | 103,566 | 99,018 | 100,194 | 105,462 |

  | 测试程序\\并发量 | 100    | 200     | 500    | 1000    | 2000    | 2500    | 3000    |
  | ---------------- | ------ | ------- | ------ | ------- | ------- | ------- | ------- |
  | 串行代码         | 25,110 | 31,890  | 19,554 | 19,368  | 24,042  | 31,104  | 27,654  |
  | 多进程代码-1进程 | 97,662 | 96,924  | 93,600 | 92,520  | 94,212  | 92,124  | 91,548  |
  | 多进程代码-2进程 | 97,344 | 102,924 | 98,052 | 109,104 | 111,108 | 114,114 | 114,900 |

- 测试2：双核Tom运行服务、单核Jerry运行webbench。服务程序的backlog为5，子进程每次收到IPC最多循环accept100次。

  | 测试程序\\并发量 | 1      | 2       | 5       | 10      | 20      | 50      |
  | ---------------- | ------ | ------- | ------- | ------- | ------- | ------- |
  | 串行代码         | 90,296 | 113,718 | 129,270 | 38,316  | 28,572  | 36,162  |
  | 多进程代码-1进程 | 74,976 | 95,208  | 110,196 | 115,986 | 114,840 | 115,782 |
  | 多进程代码-2进程 | 78,432 | 106,104 | 126,342 | 121,092 | 119,988 | 114,978 |

  | 测试程序\\并发量 | 100     | 200     | 500     | 1000    | 2000    | 2500    | 3000    |
  | ---------------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- |
  | 串行代码         | 35,412  | 21,174  | 22,320  | 29,970  | 18,864  | 25,248  | 32,082  |
  | 多进程代码-1进程 | 116,562 | 118,518 | 120,516 | 119,250 | 117,570 | 101,904 | 122,712 |
  | 多进程代码-2进程 | 112,992 | 114,798 | 122,196 | 135,732 | 157,794 | 179,292 | 171,720 |

- 简单分析：非阻塞式IO+epoll_wait事件处理（“多进程代码-1进程”）相比串行已经能抗住比较高的并发（如20220722分析中提到的，writev和close耗时更小，原因待探索）。**当前场景**（类似20220722中用`strace -c`分析，感觉writev、close和IPC的sendto应该是瓶颈，所以场景应该是IO密集型，CPU计算量少）下，多进程在单核机器上无明显性能优势，多进程在多核+适当的进程数情况下似乎能体现优势。但可能因为程序总体计算量不够（即IO和CPU可重叠的“计算”不多），所以优势不明显。



#### 问题4（20220722）：多进程随并发量增加QPS不提升甚至下降

描述：测试场景，2进程、backlog为5、每个进程循环accept最多100次、父进程ET模式通知子进程作accept，分别用`webbench -t 10 -c <nc>`（其中`nc`取2、20、200、2000）

2022-07-24-猜想：此前测试场景为单核单机，并发量上升，进程上下文切换花费更多时间（可看`vmstat`的`r`列是否远超CPU核数）。



#### 问题5（20220722）：单核多进程能提高QPS的适用场景是什么？

描述：考虑下限制QPS的因素大概有哪些？

2022-07-24-回复：通过问题3的分析，单核多进程增加了CPU在进程上下文时的开销，可能适得其反（所以进程池中进程数量也有考究）。所以如果要用多进程提升QPS，应该需要服务器有多核可用，而且程序有一定的CPU计算量才可能通过增加进程数显著提高QPS（CPU密集型适合同步处理，多进程本质是做到同时为多个请求作CPU计算）。IO密集型适合异步计算，增加进程数不一定能显著缩短处理时间，反而使用epoll机制防止阻塞会有更好的效果。引用陈硕的话：

> （《Linux多线程服务器编程》第3.5节和第3.6节）
>
> 多线程程序有性能优势吗？
> 前面我说，无论是IO bound还是CPU bound的服务，多线程都没有什么绝对意义上的性能优势。这句话是说，如果用很少的CPU负载就能让IO跑满，或者用很少的IO流量就能让CPU跑满，那么多线程没啥用处。举例来说：
>
> 对于静态Web服务器，或者FTP服务器，CPU的负载较轻，主要瓶颈在磁盘IO和网络IO方面。这时候往往一个单线程的程序（模式1-1-1个单线程的进程）就能撑满IO。用多线程并不能提高吞吐量，因为IO硬件容量已经饱和了。同理，这时增加CPU数目也不能提高吞吐量。
>
> ......
>
> 我认为多线程的适用场景是：提高响应速度，让IO和“计算”相互重叠，降低latency。虽然多线程不能提高绝对性能，但能提高平均响应性能。
>
> 一个程序要做成多线程的，大致要满足：
>
> 有多个CPU可用。单核机器上多线程没有性能优势（但或许能简化并发业务逻辑的实现）。
>
> ......
>
> latency和throughput同样重要，不是逻辑简单的IO bound或CPUbound程序。换言之，程序要有相当的计算量。
> 利用异步操作。比如logging。
>
> ......
>
> 2．多线程能提高并发度吗？
> 如果指的是“并发连接数”，则不能。由问题1可知，假如单纯采用thread per connection的模型，那么并发连接数最多300，这远远低于基于事件的单线程程序所能轻松达到的并发连接数（几千乃至上万，甚至几万）。所谓“基于事件”，指的是用IO
> multiplexing event loop的编程模型，又称Reactor模式，在前文中已有介绍。那么采用前文中推荐的one loop per thread呢？至少不逊于单线程程序。实际上单个event loop处理1万个并发长连接并不罕见，一个multiloop的多线程程序应该能轻松支持5万并发链接。
>
> 小结：thread per connection不适合高并发场合，其scalability不佳。one loop per thread的并发度足够大，且与CPU数目成正比。

